<?xml version="1.0" encoding="UTF-8"?>
<mods:mods xmlns:mods="http://www.loc.gov/mods/v3" xmlns="http://www.loc.gov/mods/v3" xmlns:etd="http://www.ndltd.org/standards/metadata/etdms/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xs="http://www.w3.org/2001/XMLSchema" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-5.xsd" version="3.5">
  <mods:titleInfo>
    <mods:title>On the Robustness of Object Detection Based Deep Learning Models</mods:title>
  </mods:titleInfo>
  <mods:name type="personal">
    <mods:namePart type="given">Matthew</mods:namePart>
    <mods:namePart type="family">Seals</mods:namePart>
    <mods:role>
      <mods:roleTerm authority="marcrelator" type="text" valueURI="http://id.loc.gov/vocabulary/relators/aut">Author</mods:roleTerm>
    </mods:role>
  </mods:name>
  <mods:originInfo>
    <mods:dateCreated encoding="w3cdtf">2019-05-18T18:33:41-04:00</mods:dateCreated>
    <mods:dateIssued keyDate="yes" encoding="edtf">2019-08</mods:dateIssued>
  </mods:originInfo>
  <mods:recordInfo displayLabel="Submission">
    <mods:recordCreationDate encoding="w3cdtf">2018-10-01T17:00:51-04:00</mods:recordCreationDate>
    <mods:recordChangeDate keyDate="yes" encoding="w3cdtf">2018-10-01T17:00:51-04:00</mods:recordChangeDate>
    <mods:recordChangeDate keyDate="yes" encoding="w3cdtf">2019-05-02T15:38:04-04:00</mods:recordChangeDate>
    <mods:recordChangeDate keyDate="yes" encoding="w3cdtf">2019-05-18T18:33:41-04:00</mods:recordChangeDate>
  </mods:recordInfo>
  <mods:extension>
    <etd:degree>
      <etd:level>Masters (pre-doctoral)</etd:level>
      <etd:name>Master of Science</etd:name>
      <etd:discipline>Computer Engineering</etd:discipline>
      <etd:grantor>University of Tennessee</etd:grantor>
    </etd:degree>
  </mods:extension>
  <mods:name>
    <mods:namePart type="given">Mongi</mods:namePart>
    <mods:namePart type="family">Abidi</mods:namePart>
    <mods:namePart type="termsOfAddress"/>
    <mods:role>
      <mods:roleTerm authority="marcrelator" type="text" valueURI="http://id.loc.gov/vocabulary/relators/ths">Thesis advisor</mods:roleTerm>
    </mods:role>
  </mods:name>
  <mods:name>
    <mods:namePart type="given">Hairong</mods:namePart>
    <mods:namePart type="family">Qi</mods:namePart>
    <mods:namePart type="termsOfAddress"/>
    <mods:role>
      <mods:roleTerm type="text">Committee member</mods:roleTerm>
    </mods:role>
  </mods:name>
  <mods:name>
    <mods:namePart type="given">Qing</mods:namePart>
    <mods:namePart type="family">Cao</mods:namePart>
    <mods:namePart type="termsOfAddress"/>
    <mods:role>
      <mods:roleTerm type="text">Committee member</mods:roleTerm>
    </mods:role>
  </mods:name>
  <mods:typeOfResource>text</mods:typeOfResource>
  <mods:genre authority="lcgft" valueURI="http://id.loc.gov/authorities/genreForms/gf2014026039">Academic theses</mods:genre>
  <mods:genre authority="coar" valueURI="http://purl.org/coar/resource_type/c_bdcc">masters thesis</mods:genre>
  <mods:language>
    <mods:languageTerm authority="iso639-2b" type="code">eng</mods:languageTerm>
  </mods:language>
  <mods:abstract>Object detection is one of the most popular areas in the field of computer vision and deep learning. Several advances have been reported in the literature showing promising object detection results. However, most of these results use databases of images that have been collected under almost ideal conditions and tested with input images mostly not representative of real life imagery. When tested with challenging data, most of these object detection models break down.&#13;
The objective of this work is to quantify the performance of the most recent object detection models in the presence of realistic degradation in the form of differing levels of brightness, saturation, contrast, Gaussian blur, image size, sharpness, Gaussian noise, speckle noise, and salt and pepper noise. We have selected Faster RCNN as a typical model that is representative of the state of the art. We have used a binary class dataset from our laboratory for testing: Aphylla. We have also selected a popular multi-class dataset widely used by the community for our work: VOC2007.&#13;
We have conducted the following experiments (1) ran the model on the original pristine dataset and recorded the mAP score result, (2) ran the model on nine methods of degradation with 12 levels in each and recorded the mAP score results, and (3) compared the degradation results to one another to determine the model robustness. These experiments led to the clustering of the degradation models into three categories: high, medium, and low impact. These categories are based on the fluctuations within the results. The first class containing brightness and contrast resembles a Gaussian-like bell shaped curve with a plateau at the top. The second cluster contains Gaussian blur, image size, and all three types of noise resembles an exponential decay. The third category contains saturation and sharpness and has shown a small reduction in performance, which stays mostly uniform throughout the range.&#13;
The value of this research comes from studying the results and providing consistent guidance to the user as to which level of image degradation needs to be dealt with at a pre-processing stage to alleviate the drop in performance.</mods:abstract>
  <mods:note displayLabel="Submitted Comment"/>
  <mods:relatedItem type="series">
    <mods:titleInfo>
      <mods:title>Graduate Theses and Dissertations</mods:title>
    </mods:titleInfo>
  </mods:relatedItem>
  <mods:note displayLabel="Keywords Submitted by Author">Faster RCNN, sensitivity analysis, deep learning, object detection, degradation model, robustness</mods:note>
  <mods:accessCondition type="local rights statement">Unless otherwise noted, (c) 2017 The Author(s).</mods:accessCondition>
  <mods:note displayLabel="Copyright holder">author</mods:note>
  <mods:physicalDescription>
    <mods:note displayLabel="Publication Status">PUBLISHED</mods:note>
  </mods:physicalDescription>
</mods:mods>